import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.patches as mpatches
from matplotlib.ticker import FormatStrFormatter

import string

import numpy as np
import pandas as pd
import itertools


from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes
from mpl_toolkits.axes_grid1.inset_locator import mark_inset

energy_dict_uop = {'seq_.75_mean_2': (11912.87, 8917100),
                   'seq_.8_mean_2': (11954.3, 9137673),
                   'seq_.85_mean_2': (11469.05, 8513780),
                   'seq_.9_mean_2': (11735.16, 8744287),
                   'seq_.925_mean_2': (11505.15, 7909134),
                   'seq_.95_mean_2': (12317.34, 10338762),
                   'seq_.975_mean_2': (11348.61, 8888105),
                   'seq_.75_mean_3': (13656.01, 9663739),
                   'seq_.8_mean_3': (13321.48, 9486153),
                   'seq_.85_mean_3': (13201.72, 9805999),
                   'seq_.9_mean_3': (12917.15, 9642550),
                   'seq_.925_mean_3': (13000.55, 9755538),
                   'seq_.95_mean_3': (12544.21, 9489675),
                   'seq_.975_mean_3': (12116.87, 8970364),
                   'seq_.75_mean_4': (14282.56, 9394460),
                   'seq_.8_mean_4': (13759.81, 8860815),
                   'seq_.85_mean_4': (13874.28, 9715765),
                   'seq_.9_mean_4': (13447.65, 9516164),
                   'seq_.925_mean_4': (13508.88, 9569907),
                   'seq_.95_mean_4': (13683, 10338331),
                   'seq_.975_mean_4': (12950.7, 8894224),
                   'seq_.75_mean_6': (15749.02, 9515371),
                   'seq_.8_mean_6': (15496.21, 9556062),
                   'seq_.85_mean_6': (14934.46, 8957525),
                   'seq_.9_mean_6': (14826.35, 9330815),
                   'seq_.925_mean_6': (14989.19, 9673001),
                   'seq_.95_mean_6': (14698.66, 9684055),
                   'seq_.975_mean_6': (14144.41, 8872488),
                   'seq_.75_mean_8': (16531.57, 9338643),
                   'seq_.8_mean_8': (16451.46, 9830171),
                   'seq_.85_mean_8': (15905.04, 9204726),
                   'seq_.9_mean_8': (15662.39, 9286740),
                   'seq_.925_mean_8': (15952.77, 10267846),
                   'seq_.95_mean_8': (15812.98, 10265753),
                   'seq_.975_mean_8': (15169.62, 8793995),
                   'seq_.75_min_2': (13810.61, 15164795),
                   'seq_.8_min_2': (14403.67, 17184272),
                   'seq_.85_min_2': (13512.49, 14875289),
                   'seq_.9_min_2': (14449.15, 17607187),
                   'seq_.925_min_2': (14088.85, 16338061),
                   'seq_.95_min_2': (16273.21, 24321205),
                   'seq_.975_min_2': (13517.05, 15705079),
                   'seq_.75_min_3': (15829.9, 17450961),
                   'seq_.8_min_3': (15523.94, 17058880),
                   'seq_.85_min_3': (15638.09, 17985663),
                   'seq_.9_min_3': (15451.57, 18105931),
                   'seq_.925_min_3': (16355.34, 21812599),
                   'seq_.95_min_3': (15902.63, 20938047),
                   'seq_.975_min_3': (14650.18, 17521547),
                   'seq_.75_min_4': (16436.36, 16813704),
                   'seq_.8_min_4': (15886.83, 15884908),
                   'seq_.85_min_4': (16190.25, 17351541),
                   'seq_.9_min_4': (15950.3, 17146680),
                   'seq_.925_min_4': (16519.28, 19548632),
                   'seq_.95_min_4': (17196.99, 22456840),
                   'seq_.975_min_4': (15156.83, 16064575),
                   'seq_.75_min_6': (18069.73, 17212590),
                   'seq_.8_min_6': (17845.53, 17402997),
                   'seq_.85_min_6': (17179.41, 15998839),
                   'seq_.9_min_6': (17277.98, 16867733),
                   'seq_.925_min_6': (17912.81, 19079887),
                   'seq_.95_min_6': (18318.68, 21803920),
                   'seq_.975_min_6': (16339.37, 16057445),
                   'seq_.75_min_8': (18244.3, 16974963),
                   'seq_.8_min_8': (18262.34, 17938690),
                   'seq_.85_min_8': (17658.98, 16546115),
                   'seq_.9_min_8': (17536.3, 16968750),
                   'seq_.925_min_8': (18174.61, 20340703),
                   'seq_.95_min_8': (18462.06, 23115510),
                   'seq_.975_min_8': (17023.45, 16870874),
                   'joint_.0_mean_2': (12188.09, 8878557),
                   'joint_.75_mean_2': (11850.75, 8454918),
                   'joint_.8_mean_2': (11905.41, 8867373),
                   'joint_.85_mean_2': (11635.77, 7969648),
                   'joint_.9_mean_2': (11615.93, 8084851),
                   'joint_.925_mean_2': (11934.39, 8877148),
                   'joint_.95_mean_2': (12166.18, 9624558),
                   'joint_.975_mean_2': (11286.33, 8402387),
                   'joint_.0_mean_3': (16598.64, 13005553),
                   'joint_.75_mean_3': (13783.91, 9997687),
                   'joint_.8_mean_3': (13241.06, 9211656),
                   'joint_.85_mean_3': (13064.02, 9466970),
                   'joint_.9_mean_3': (12726.95, 9173979),
                   'joint_.925_mean_3': (12524.95, 8850015),
                   'joint_.95_mean_3': (12595.4, 9344971),
                   'joint_.975_mean_3': (11930.48, 8653170),
                   'joint_.0_mean_4': (18761.01, 13505969),
                   'joint_.75_mean_4': (14305.07, 9452831),
                   'joint_.8_mean_4': (13969.68, 9288429),
                   'joint_.85_mean_4': (13498.16, 8754213),
                   'joint_.9_mean_4': (13367.67, 9083992),
                   'joint_.925_mean_4': (12968.7, 8478308),
                   'joint_.95_mean_4': (13715.89, 9948328),
                   'joint_.975_mean_4': (12638.25, 8396357),
                   'joint_.0_mean_6': (21609.07, 15052728),
                   'joint_.75_mean_6': (15875.36, 9832372),
                   'joint_.8_mean_6': (15557.83, 9774588),
                   'joint_.85_mean_6': (14987.47, 8996954),
                   'joint_.9_mean_6': (14817.01, 9118361),
                   'joint_.925_mean_6': (14882.19, 9529518),
                   'joint_.95_mean_6': (14690.33, 9432734),
                   'joint_.975_mean_6': (14115.28, 8798259),
                   'joint_.0_mean_8': (22417.2, 15444636),
                   'joint_.75_mean_8': (16852.6, 10296596),
                   'joint_.8_mean_8': (16368.21, 9668960),
                   'joint_.85_mean_8': (15811.38, 8935132),
                   'joint_.9_mean_8': (15620.61, 9089176),
                   'joint_.925_mean_8': (15391.88, 8946060),
                   'joint_.95_mean_8': (15647.85, 9751957),
                   'joint_.975_mean_8': (15019.87, 8686368),
                   'joint_.0_min_2': (10730.57, 2800074),
                   'joint_.75_min_2': (13879.56, 15175815),
                   'joint_.8_min_2': (13933.41, 15575906),
                   'joint_.85_min_2': (13868.9, 15202585),
                   'joint_.9_min_2': (14080.89, 15926602),
                   'joint_.925_min_2': (15011.01, 19032241),
                   'joint_.95_min_2': (16957.78, 26261886),
                   'joint_.975_min_2': (13524.67, 15712138),
                   'joint_.0_min_3': (19245.82, 24633639),
                   'joint_.75_min_3': (16057.08, 18111480),
                   'joint_.8_min_3': (15445.35, 16755634),
                   'joint_.85_min_3': (15420.05, 17352802),
                   'joint_.9_min_3': (15195.56, 17309998),
                   'joint_.925_min_3': (15048.14, 16908315),
                   'joint_.95_min_3': (15790.87, 20378442),
                   'joint_.975_min_3': (14015.96, 15605747),
                   'joint_.0_min_4': (16823.41, 4183657),
                   'joint_.75_min_4': (16501.53, 16958996),
                   'joint_.8_min_4': (16143.84, 16564649),
                   'joint_.85_min_4': (15657.9, 15759749),
                   'joint_.9_min_4': (15702.22, 16392022),
                   'joint_.925_min_4': (15347.56, 15910302),
                   'joint_.95_min_4': (17756.65, 24155418),
                   'joint_.975_min_4': (14751.49, 15282601),
                   'joint_.0_min_6': (19261.11, 4167105),
                   'joint_.75_min_6': (18218.98, 17683862),
                   'joint_.8_min_6': (17948.18, 17793957),
                   'joint_.85_min_6': (17275.6, 16197043),
                   'joint_.9_min_6': (17308.87, 16914328),
                   'joint_.925_min_6': (17789.95, 18987480),
                   'joint_.95_min_6': (18079.21, 20694851),
                   'joint_.975_min_6': (16330.11, 15928351),
                   'joint_.0_min_8': (20859.01, 4567443),
                   'joint_.75_min_8': (18676.63, 18674138),
                   'joint_.8_min_8': (18160.63, 17577734),
                   'joint_.85_min_8': (17479.87, 15886666),
                   'joint_.9_min_8': (17549.66, 16908779),
                   'joint_.925_min_8': (17397.25, 17048262),
                   'joint_.95_min_8': (18348.43, 22308402),
                   'joint_.975_min_8': (16853.95, 16385558), }

energy_dict_ubm = {
    'seq_.75_mean_2': (12741, 8917100),
    'seq_.8_mean_2': (12785.7, 9137673),
    'seq_.85_mean_2': (12299.95, 8513780),
    'seq_.9_mean_2': (12569.22, 8744287),
    'seq_.925_mean_2': (12340.52, 7909134),
    'seq_.95_mean_2': (13160.28, 10338762),
    'seq_.975_mean_2': (12178.89, 8888105),
    'seq_.75_mean_3': (14639.63, 9663739),
    'seq_.8_mean_3': (14303.45, 9486153),
    'seq_.85_mean_3': (14182.82, 9805999),
    'seq_.9_mean_3': (13899.75, 9642550),
    'seq_.925_mean_3': (13985.7, 9755538),
    'seq_.95_mean_3': (13524.63, 9489675),
    'seq_.975_mean_3': (13090.55, 8970364),
    'seq_.75_mean_4': (15407.51, 9394460),
    'seq_.8_mean_4': (14882.53, 8860815),
    'seq_.85_mean_4': (14997.27, 9715765),
    'seq_.9_mean_4': (14567.86, 9516164),
    'seq_.925_mean_4': (14632.91, 9569907),
    'seq_.95_mean_4': (14809.15, 10338331),
    'seq_.975_mean_4': (14069.4, 8894224),
    'seq_.75_mean_6': (17156.04, 9515371),
    'seq_.8_mean_6': (16901.3, 9556062),
    'seq_.85_mean_6': (16336.49, 8957525),
    'seq_.9_mean_6': (16229.95, 9330815),
    'seq_.925_mean_6': (16397.03, 9673001),
    'seq_.95_mean_6': (16102.25, 9684055),
    'seq_.975_mean_6': (15541.19, 8872488),
    'seq_.75_mean_8': (18217.62, 9338643),
    'seq_.8_mean_8': (18138.55, 9830171),
    'seq_.85_mean_8': (17587.57, 9204726),
    'seq_.9_mean_8': (17344.98, 9286740),
    'seq_.925_mean_8': (17641.08, 10267846),
    'seq_.95_mean_8': (17500.32, 10265753),
    'seq_.975_mean_8': (16850.12, 8793995),
    'seq_.75_min_2': (14653.78, 15164795),
    'seq_.8_min_2': (15256.19, 17184272),
    'seq_.85_min_2': (14360.99, 14875289),
    'seq_.9_min_2': (15302.94, 17607187),
    'seq_.925_min_2': (14944.39, 16338061),
    'seq_.95_min_2': (17143.24, 24321205),
    'seq_.975_min_2': (14363.32, 15705079),
    'seq_.75_min_3': (16830.88, 17450961),
    'seq_.8_min_3': (16523.66, 17058880),
    'seq_.85_min_3': (16637.19, 17985663),
    'seq_.9_min_3': (16451.16, 18105931),
    'seq_.925_min_3': (17360.16, 21812599),
    'seq_.95_min_3': (16903.99, 20938047),
    'seq_.975_min_3': (15639.52, 17521547),
    'seq_.75_min_4': (17578.19, 16813704),
    'seq_.8_min_4': (17026.56, 15884908),
    'seq_.85_min_4': (17330.07, 17351541),
    'seq_.9_min_4': (17090.63, 17146680),
    'seq_.925_min_4': (17663.1, 19548632),
    'seq_.95_min_4': (18345.06, 22456840),
    'seq_.975_min_4': (16291.82, 16064575),
    'seq_.75_min_6': (19494.51, 17212590),
    'seq_.8_min_6': (19267.64, 17402997),
    'seq_.85_min_6': (18599.28, 15998839),
    'seq_.9_min_6': (18699.73, 16867733),
    'seq_.925_min_6': (19339.77, 19079887),
    'seq_.95_min_6': (19743.57, 21803920),
    'seq_.975_min_6': (17750.66, 16057445),
    'seq_.75_min_8': (19947.71, 16974963),
    'seq_.8_min_8': (19966.12, 17938690),
    'seq_.85_min_8': (19359.39, 16546115),
    'seq_.9_min_8': (19237.68, 16968750),
    'seq_.925_min_8': (19882.79, 20340703),
    'seq_.95_min_8': (20172.28, 23115510),
    'seq_.975_min_8': (18721.16, 16870874),
    'joint_.0_mean_2': (13015.37, 8878557),
    'joint_.75_mean_2': (12678.99, 8454918),
    'joint_.8_mean_2': (12734.81, 8867373),
    'joint_.85_mean_2': (12466.96, 7969648),
    'joint_.9_mean_2': (12451.1, 8084851),
    'joint_.925_mean_2': (12775.13, 8877148),
    'joint_.95_mean_2': (13008.26, 9624558),
    'joint_.975_mean_2': (12118.05, 8402387),
    'joint_.0_mean_3': (17594.45, 13005553),
    'joint_.75_mean_3': (14767.64, 9997687),
    'joint_.8_mean_3': (14222.44, 9211656),
    'joint_.85_mean_3': (14044.32, 9466970),
    'joint_.9_mean_3': (13706.91, 9173979),
    'joint_.925_mean_3': (13505.12, 8850015),
    'joint_.95_mean_3': (13575.76, 9344971),
    'joint_.975_mean_3': (12901.98, 8653170),
    'joint_.0_mean_4': (19906.35, 13505969),
    'joint_.75_mean_4': (15430.38, 9452831),
    'joint_.8_mean_4': (15093.35, 9288429),
    'joint_.85_mean_4': (14618.13, 8754213),
    'joint_.9_mean_4': (14488.72, 9083992),
    'joint_.925_mean_4': (14086.59, 8478308),
    'joint_.95_mean_4': (14843.87, 9948328),
    'joint_.975_mean_4': (13753.1, 8396357),
    'joint_.0_mean_6': (23041.3, 15052728),
    'joint_.75_mean_6': (17282.79, 9832372),
    'joint_.8_mean_6': (16962.4, 9774588),
    'joint_.85_mean_6': (16389.79, 8996954),
    'joint_.9_mean_6': (16220.18, 9118361),
    'joint_.925_mean_6': (16287.18, 9529518),
    'joint_.95_mean_6': (16094.66, 9432734),
    'joint_.975_mean_6': (15512.31, 8798259),
    'joint_.0_mean_8': (24130.99, 15444636),
    'joint_.75_mean_8': (18539.85, 10296596),
    'joint_.8_mean_8': (18053.74, 9668960),
    'joint_.85_mean_8': (17492.46, 8935132),
    'joint_.9_mean_8': (17302.96, 9089176),
    'joint_.925_mean_8': (17072.82, 8946060),
    'joint_.95_mean_8': (17333.37, 9751957),
    'joint_.975_mean_8': (16697.09, 8686368),
    'joint_.0_min_2': (11550.74, 2800074),
    'joint_.75_min_2': (14723.75, 15175815),
    'joint_.8_min_2': (14778.53, 15575906),
    'joint_.85_min_2': (14718.37, 15202585),
    'joint_.9_min_2': (14935.48, 15926602),
    'joint_.925_min_2': (15873.03, 19032241),
    'joint_.95_min_2': (17832.58, 26261886),
    'joint_.975_min_2': (14371.95, 15712138),
    'joint_.0_min_3': (20258.86, 24633639),
    'joint_.75_min_3': (17058.06, 18111480),
    'joint_.8_min_3': (16444.42, 16755634),
    'joint_.85_min_3': (16417.49, 17352802),
    'joint_.9_min_3': (16192.85, 17309998),
    'joint_.925_min_3': (16047.58, 16908315),
    'joint_.95_min_3': (16790.86, 20378442),
    'joint_.975_min_3': (15001.12, 15605747),
    'joint_.0_min_4': (17961.48, 4183657),
    'joint_.75_min_4': (17644.15, 16958996),
    'joint_.8_min_4': (17284.93, 16564649),
    'joint_.85_min_4': (16795.41, 15759749),
    'joint_.9_min_4': (16841.21, 16392022),
    'joint_.925_min_4': (16483.53, 15910302),
    'joint_.95_min_4': (18909.73, 24155418),
    'joint_.975_min_4': (15881.49, 15282601),
    'joint_.0_min_6': (20685.91, 4167105),
    'joint_.75_min_6': (19644.16, 17683862),
    'joint_.8_min_6': (19369.71, 17793957),
    'joint_.85_min_6': (18696.27, 16197043),
    'joint_.9_min_6': (18729.58, 16914328),
    'joint_.925_min_6': (19213.97, 18987480),
    'joint_.95_min_6': (19503.13, 20694851),
    'joint_.975_min_6': (17742.11, 15928351),
    'joint_.0_min_8': (22565.06, 4567443),
    'joint_.75_min_8': (20381.1, 18674138),
    'joint_.8_min_8': (19863.71, 17577734),
    'joint_.85_min_8': (19178.16, 15886666),
    'joint_.9_min_8': (19250.38, 16908779),
    'joint_.925_min_8': (19097.46, 17048262),
    'joint_.95_min_8': (20056.62, 22308402),
    'joint_.975_min_8': (18549.48, 16385558),
}

energy_dict_cp = {
    'seq_.75_mean_2': (9586.57, 8917100),
    'seq_.8_mean_2': (9270.14, 9137673),
    'seq_.85_mean_2': (8794.04, 8513780),
    'seq_.9_mean_2': (9055.18, 8744287),
    'seq_.925_mean_2': (8479.22, 7909134),
    'seq_.95_mean_2': (10050.29, 10338762),
    'seq_.975_mean_2': (8190.77, 8888105),
    'seq_.75_mean_3': (11478.23, 9663739),
    'seq_.8_mean_3': (11064.6, 9486153),
    'seq_.85_mean_3': (10618.5, 9805999),
    'seq_.9_mean_3': (10289.95, 9642550),
    'seq_.925_mean_3': (10041.87, 9755538),
    'seq_.95_mean_3': (9454.5, 9489675),
    'seq_.975_mean_3': (8901.37, 8970364),
    'seq_.75_mean_4': (11720.62, 9394460),
    'seq_.8_mean_4': (11080.81, 8860815),
    'seq_.85_mean_4': (11241.69, 9715765),
    'seq_.9_mean_4': (10708.84, 9516164),
    'seq_.925_mean_4': (10766.64, 9569907),
    'seq_.95_mean_4': (11357.67, 10338331),
    'seq_.975_mean_4': (9685.62, 8894224),
    'seq_.75_mean_6': (13142.18, 9515371),
    'seq_.8_mean_6': (13169.04, 9556062),
    'seq_.85_mean_6': (12218.68, 8957525),
    'seq_.9_mean_6': (12023.7, 9330815),
    'seq_.925_mean_6': (11878.86, 9673001),
    'seq_.95_mean_6': (11450.22, 9684055),
    'seq_.975_mean_6': (10668.71, 8872488),
    'seq_.75_mean_8': (13877.24, 9338643),
    'seq_.8_mean_8': (14127.58, 9830171),
    'seq_.85_mean_8': (13118.09, 9204726),
    'seq_.9_mean_8': (12770.96, 9286740),
    'seq_.925_mean_8': (12745.57, 10267846),
    'seq_.95_mean_8': (13356.86, 10265753),
    'seq_.975_mean_8': (11677.55, 8793995),
    'seq_.75_min_2': (11660.56, 15164795),
    'seq_.8_min_2': (12920.2, 17184272),
    'seq_.85_min_2': (11557.79, 14875289),
    'seq_.9_min_2': (12428.11, 17607187),
    'seq_.925_min_2': (11889.91, 16338061),
    'seq_.95_min_2': (15515.77, 24321205),
    'seq_.975_min_2': (11294.91, 15705079),
    'seq_.75_min_3': (14637.91, 17450961),
    'seq_.8_min_3': (14169.2, 17058880),
    'seq_.85_min_3': (13842.8, 17985663),
    'seq_.9_min_3': (13491.29, 18105931),
    'seq_.925_min_3': (14363.76, 21812599),
    'seq_.95_min_3': (13803.11, 20938047),
    'seq_.975_min_3': (12404.06, 17521547),
    'seq_.75_min_4': (15246.19, 16813704),
    'seq_.8_min_4': (14492.12, 15884908),
    'seq_.85_min_4': (14370.63, 17351541),
    'seq_.9_min_4': (13966.32, 17146680),
    'seq_.925_min_4': (14480.66, 19548632),
    'seq_.95_min_4': (15106.08, 22456840),
    'seq_.975_min_4': (12905.18, 16064575),
    'seq_.75_min_6': (16891.38, 17212590),
    'seq_.8_min_6': (16508.74, 17402997),
    'seq_.85_min_6': (15282.8, 15998839),
    'seq_.9_min_6': (15234.56, 16867733),
    'seq_.925_min_6': (15845.29, 19079887),
    'seq_.95_min_6': (16127.46, 21803920),
    'seq_.975_min_6': (13972.6, 16057445),
    'seq_.75_min_8': (17074.04, 16974963),
    'seq_.8_min_8': (16929.05, 17938690),
    'seq_.85_min_8': (15755.52, 16546115),
    'seq_.9_min_8': (15432.25, 16968750),
    'seq_.925_min_8': (16050.61, 20340703),
    'seq_.95_min_8': (16240.8, 23115510),
    'seq_.975_min_8': (14650.6, 16870874),
    'joint_.0_mean_2': (9633.09, 8878557),
    'joint_.75_mean_2': (9188.59, 8454918),
    'joint_.8_mean_2': (9599.99, 8867373),
    'joint_.85_mean_2': (8996.22, 7969648),
    'joint_.9_mean_2': (8931.11, 8084851),
    'joint_.925_mean_2': (8931.81, 8877148),
    'joint_.95_mean_2': (9892.99, 9624558),
    'joint_.975_mean_2': (8131.4, 8402387),
    'joint_.0_mean_3': (16550.95, 13005553),
    'joint_.75_mean_3': (11633.41, 9997687),
    'joint_.8_mean_3': (10981.93, 9211656),
    'joint_.85_mean_3': (10473.73, 9466970),
    'joint_.9_mean_3': (10065.34, 9173979),
    'joint_.925_mean_3': (9808.11, 8850015),
    'joint_.95_mean_3': (9510.19, 9344971),
    'joint_.975_mean_3': (8735.37, 8653170),
    'joint_.0_mean_4': (18927.44, 13568473),
    'joint_.75_mean_4': (11736.36, 9452831),
    'joint_.8_mean_4': (11663.92, 9288429),
    'joint_.85_mean_4': (10833.61, 8754213),
    'joint_.9_mean_4': (10653.36, 9083992),
    'joint_.925_mean_4': (10157.34, 8478308),
    'joint_.95_mean_4': (11414.34, 9948328),
    'joint_.975_mean_4': (9354.43, 8396357),
    'joint_.0_mean_6': (22238.79, 15066882),
    'joint_.75_mean_6': (13646.53, 9832372),
    'joint_.8_mean_6': (13260.15, 9774588),
    'joint_.85_mean_6': (12261.14, 8996954),
    'joint_.9_mean_6': (12011.2, 9118361),
    'joint_.925_mean_6': (12039.26, 9529518),
    'joint_.95_mean_6': (11434.81, 9432734),
    'joint_.975_mean_6': (10648, 8798259),
    'joint_.0_mean_8': (23312.68, 15448231),
    'joint_.75_mean_8': (14625.96, 10296596),
    'joint_.8_mean_8': (14031.4, 9668960),
    'joint_.85_mean_8': (12984.65, 8935132),
    'joint_.9_mean_8': (12728.91, 9089176),
    'joint_.925_mean_8': (12421.09, 8946060),
    'joint_.95_mean_8': (12298.34, 9751957),
    'joint_.975_mean_8': (11473.71, 8686368),
    'joint_.0_min_2': (7980.27, 2800074),
    'joint_.75_min_2': (11731.35, 15175815),
    'joint_.8_min_2': (11797.36, 15575906),
    'joint_.85_min_2': (11943.24, 15202585),
    'joint_.9_min_2': (12005.74, 15926602),
    'joint_.925_min_2': (12997.08, 19032241),
    'joint_.95_min_2': (16208.66, 26261886),
    'joint_.975_min_2': (11305.99, 15712138),
    'joint_.0_min_3': (19618.5, 24633639),
    'joint_.75_min_3': (14897.25, 18111480),
    'joint_.8_min_3': (14088.13, 16755634),
    'joint_.85_min_3': (13621.21, 17352802),
    'joint_.9_min_3': (13209.94, 17309998),
    'joint_.925_min_3': (12978.89, 16908315),
    'joint_.95_min_3': (13662.76, 20378442),
    'joint_.975_min_3': (10953.02, 15605747),
    'joint_.0_min_4': (16789.41, 4207443),
    'joint_.75_min_4': (15312.89, 16958996),
    'joint_.8_min_4': (14783.85, 16564649),
    'joint_.85_min_4': (13799.49, 15759749),
    'joint_.9_min_4': (13684.7, 16392022),
    'joint_.925_min_4': (13192.25, 15910302),
    'joint_.95_min_4': (17071.72, 24155418),
    'joint_.975_min_4': (12459.18, 15282601),
    'joint_.0_min_6': (19682.89, 4171511),
    'joint_.75_min_6': (17069.41, 17683862),
    'joint_.8_min_6': (16634.46, 17793957),
    'joint_.85_min_6': (15381.09, 16197043),
    'joint_.9_min_6': (15268.46, 16914328),
    'joint_.925_min_6': (15683.21, 18987480),
    'joint_.95_min_6': (15852.33, 20694851),
    'joint_.975_min_6': (13961.77, 15928351),
    'joint_.0_min_8': (21600.37, 4568532),
    'joint_.75_min_8': (17568.37, 18674138),
    'joint_.8_min_8': (16825.38, 17577734),
    'joint_.85_min_8': (15545.37, 15886666),
    'joint_.9_min_8': (15459.69, 16908779),
    'joint_.925_min_8': (15200.42, 17048262),
    'joint_.95_min_8': (16124.15, 22308402),
    'joint_.975_min_8': (14453.29, 16385558),
}

# energy, cycles
energy_dict_rle = {
    'seq_.75_mean_2': (9552.750000000002, 8917100.0),
    'seq_.8_mean_2': (9240.09, 9137673.0),
    'seq_.85_mean_2': (8666.009999999998, 8513780.0),
    'seq_.9_mean_2': (9005.69, 8744287.0),
    'seq_.925_mean_2': (8459.67, 7909134.0),
    'seq_.95_mean_2': (10010.320000000002, 10338762.0),
    'seq_.975_mean_2': (8168.55, 8888105.0),
    'seq_.75_mean_3': (11083.48, 9663739.0),
    'seq_.8_mean_3': (11032.210000000003, 9486153.0),
    'seq_.85_mean_3': (10572.240000000002, 9805999.0),
    'seq_.9_mean_3': (10234.630000000001, 9642550.0),
    'seq_.925_mean_3': (9996.050000000001, 9755538.0),
    'seq_.95_mean_3': (9432.380000000001, 9489675.0),
    'seq_.975_mean_3': (8880.88, 8970364.0),
    'seq_.75_mean_4': (11669.359999999999, 9394460.0),
    'seq_.8_mean_4': (11055.689999999999, 8860815.0),
    'seq_.85_mean_4': (11199.88, 9715765.0),
    'seq_.9_mean_4': (10679.91, 9516164.0),
    'seq_.925_mean_4': (10708.139999999998, 9569907.0),
    'seq_.95_mean_4': (10537.619999999999, 10338331.0),
    'seq_.975_mean_4': (9664.169999999998, 8894224.0),
    'seq_.75_mean_6': (13094.76, 9515371.0),
    'seq_.8_mean_6': (13143.089999999998, 9556062.0),
    'seq_.85_mean_6': (12157.6, 8957525.0),
    'seq_.9_mean_6': (11982.22, 9330815.0),
    'seq_.925_mean_6': (12110.150000000001, 9673001.0),
    'seq_.95_mean_6': (11426.22, 9684055.0),
    'seq_.975_mean_6': (10724.170000000002, 8872488.0),
    'seq_.75_mean_8': (13824.800000000001, 9338643.0),
    'seq_.8_mean_8': (14078.31, 9830171.0),
    'seq_.85_mean_8': (13065.220000000001, 9204726.0),
    'seq_.9_mean_8': (12730.630000000001, 9286740.0),
    'seq_.925_mean_8': (12705.150000000001, 10267846.0),
    'seq_.95_mean_8': (12457.28, 10265753.0),
    'seq_.975_mean_8': (11653.579999999998, 8793995.0),
    'seq_.75_min_2': (11607.489999999998, 15164795.0),
    'seq_.8_min_2': (12864.010000000002, 17184272.0),
    'seq_.85_min_2': (11501.35, 14875289.0),
    'seq_.9_min_2': (12375.659999999996, 17607187.0),
    'seq_.925_min_2': (11848.250000000002, 16338061.0),
    'seq_.95_min_2': (15461.099999999999, 24321205.0),
    'seq_.975_min_2': (11249.159999999996, 15705079.0),
    'seq_.75_min_3': (14586.830000000002, 17450961.0),
    'seq_.8_min_3': (14138.169999999998, 17058880.0),
    'seq_.85_min_3': (13782.039999999999, 17985663.0),
    'seq_.9_min_3': (13445.460000000001, 18105931.0),
    'seq_.925_min_3': (14295.01, 21812599.0),
    'seq_.95_min_3': (13748.150000000003, 20938047.0),
    'seq_.975_min_3': (12357.879999999997, 17521547.0),
    'seq_.75_min_4': (14361.720000000003, 16813704.0),
    'seq_.8_min_4': (13716.300000000001, 15884908.0),
    'seq_.85_min_4': (14301.269999999999, 17351541.0),
    'seq_.9_min_4': (13905.520000000002, 17146680.0),
    'seq_.925_min_4': (14417.260000000002, 19548632.0),
    'seq_.95_min_4': (15055.79, 22456840.0),
    'seq_.975_min_4': (12849.889999999998, 16064575.0),
    'seq_.75_min_6': (16835.6, 17212590.0),
    'seq_.8_min_6': (15694.41, 17402997.0),
    'seq_.85_min_6': (15233.139999999998, 15998839.0),
    'seq_.9_min_6': (15180.599999999997, 16867733.0),
    'seq_.925_min_6': (15766.36, 19079887.0),
    'seq_.95_min_6': (16070.349999999999, 21803920.0),
    'seq_.975_min_6': (13916.210000000001, 16057445.0),
    'seq_.75_min_8': (16106.749999999998, 16974963.0),
    'seq_.8_min_8': (16902.849999999995, 17938690.0),
    'seq_.85_min_8': (15688.37, 16546115.0),
    'seq_.9_min_8': (15392.91, 16968750.0),
    'seq_.925_min_8': (15989.380000000003, 20340703.0),
    'seq_.95_min_8': (16204.36, 23115510.0),
    'seq_.975_min_8': (14594.550000000001, 16870874.0),
    'joint_.0_mean_2': (9593.0, 8878557.0),
    'joint_.75_mean_2': (9133.619999999999, 8454918.0),
    'joint_.8_mean_2': (9536.079999999996, 8867373.0),
    'joint_.85_mean_2': (8948.31, 7969648.0),
    'joint_.9_mean_2': (8880.56, 8084851.0),
    'joint_.925_mean_2': (8907.19, 8877148.0),
    'joint_.95_mean_2': (9849.98, 9624558.0),
    'joint_.975_mean_2': (8108.660000000001, 8402387.0),
    'joint_.0_mean_3': (16520.61, 13005553.0),
    'joint_.75_mean_3': (11578.359999999999, 9997687.0),
    'joint_.8_mean_3': (10941.84, 9211656.0),
    'joint_.85_mean_3': (10427.630000000001, 9466970.0),
    'joint_.9_mean_3': (10022.96, 9173979.0),
    'joint_.925_mean_3': (9758.510000000002, 8850015.0),
    'joint_.95_mean_3': (9486.730000000003, 9344971.0),
    'joint_.975_mean_3': (8708.529999999999, 8653170.0),
    'joint_.0_mean_4': (18857.5, 13568473.0),
    'joint_.75_mean_4': (11692.88, 9452831.0),
    'joint_.8_mean_4': (11642.96, 9288429.0),
    'joint_.85_mean_4': (10795.96, 8754213.0),
    'joint_.9_mean_4': (10606.53, 9083992.0),
    'joint_.925_mean_4': (10112.59, 8478308.0),
    'joint_.95_mean_4': (11377.020000000002, 9948328.0),
    'joint_.975_mean_4': (9333.400000000001, 8396357.0),
    'joint_.0_mean_6': (22165.68, 15066882.0),
    'joint_.75_mean_6': (13607.539999999999, 9832372.0),
    'joint_.8_mean_6': (13216.53, 9774588.0),
    'joint_.85_mean_6': (12204.17, 8996954.0),
    'joint_.9_mean_6': (11970.630000000001, 9118361.0),
    'joint_.925_mean_6': (11980.27, 9529518.0),
    'joint_.95_mean_6': (11420.02, 9432734.0),
    'joint_.975_mean_6': (10694.979999999998, 8798259.0),
    'joint_.0_mean_8': (23280.719999999998, 15448231.0),
    'joint_.75_mean_8': (14584.64, 10296596.0),
    'joint_.8_mean_8': (13977.48, 9668960.0),
    'joint_.85_mean_8': (12949.990000000002, 8935132.0),
    'joint_.9_mean_8': (12685.989999999998, 9089176.0),
    'joint_.925_mean_8': (12363.910000000002, 8946060.0),
    'joint_.95_mean_8': (12282.470000000001, 9751957.0),
    'joint_.975_mean_8': (11517.88, 8686368.0),
    'joint_.0_min_2': (7963.969999999999, 2800074.0),
    'joint_.75_min_2': (11687.31, 15175815.0),
    'joint_.8_min_2': (11743.550000000001, 15575906.0),
    'joint_.85_min_2': (11884.11, 15202585.0),
    'joint_.9_min_2': (11997.039999999999, 15926602.0),
    'joint_.925_min_2': (12931.58, 19032241.0),
    'joint_.95_min_2': (16158.140000000001, 26261886.0),
    'joint_.975_min_2': (11262.210000000001, 15712138.0),
    'joint_.0_min_3': (19558.160000000003, 24633639.0),
    'joint_.75_min_3': (14847.01, 18111480.0),
    'joint_.8_min_3': (14051.130000000001, 16755634.0),
    'joint_.85_min_3': (13550.06, 17352802.0),
    'joint_.9_min_3': (13171.300000000001, 17309998.0),
    'joint_.925_min_3': (12946.259999999998, 16908315.0),
    'joint_.95_min_3': (13629.1, 20378442.0),
    'joint_.975_min_3': (10919.33, 15605747.0),
    'joint_.0_min_4': (16758.440000000002, 4207443.0),
    'joint_.75_min_4': (14432.2, 16958996.0),
    'joint_.8_min_4': (14738.070000000002, 16564649.0),
    'joint_.85_min_4': (13743.66, 15759749.0),
    'joint_.9_min_4': (13646.3, 16392022.0),
    'joint_.925_min_4': (13132.81, 15910302.0),
    'joint_.95_min_4': (15667.059999999998, 24155418.0),
    'joint_.975_min_4': (12400.509999999998, 15282601.0),
    'joint_.0_min_6': (19662.480000000003, 4171511.0),
    'joint_.75_min_6': (17002.91, 17683862.0),
    'joint_.8_min_6': (15812.680000000002, 17793957.0),
    'joint_.85_min_6': (15337.479999999998, 16197043.0),
    'joint_.9_min_6': (15205.09, 16914328.0),
    'joint_.925_min_6': (15618.769999999999, 18987480.0),
    'joint_.95_min_6': (15823.4, 20694851.0),
    'joint_.975_min_6': (13912.989999999998, 15928351.0),
    'joint_.0_min_8': (21582.49, 4568532.0),
    'joint_.75_min_8': (17514.809999999998, 18674138.0),
    'joint_.8_min_8': (16785.52, 17577734.0),
    'joint_.85_min_8': (15489.720000000001, 15886666.0),
    'joint_.9_min_8': (15400.82, 16908779.0),
    'joint_.925_min_8': (15134.12, 17048262.0),
    'joint_.95_min_8': (16073.86, 22308402.0),
    'joint_.975_min_8': (14405.11, 16385558.0),
}

acc_dict = {
    'seq_0.75_2': 0.9618,
    'seq_0.8_2': 0.9688,
    'seq_0.85_2': 0.9653,
    'seq_0.9_2': 0.9271,
    'seq_0.95_2': 0.7396,
    'seq_0.75_3': 0.9757,
    'seq_0.8_3': 0.9757,
    'seq_0.85_3': 0.9722,
    'seq_0.9_3': 0.9618,
    'seq_0.95_3': 0.7639,
    'seq_0.75_4': 0.9757,
    'seq_0.8_4': 0.9792,
    'seq_0.85_4': 0.9688,
    'seq_0.9_4': 0.9583,
    'seq_0.95_4': 0.7361,
    'seq_0.75_6': 0.9722,
    'seq_0.8_6': 0.9722,
    'seq_0.85_6': 0.9653,
    'seq_0.9_6': 0.9653,
    'seq_0.95_6': 0.7153,
    'seq_0.75_8': 0.9688,
    'seq_0.8_8': 0.9757,
    'seq_0.85_8': 0.9653,
    'seq_0.9_8': 0.9653,
    'seq_0.95_8': 0.7222,
    'joint_0_2': 0.9757,
    'joint_0.75_2': 0.9722,
    'joint_0.8_2': 0.9722,
    'joint_0.85_2': 0.9583,
    'joint_0.9_2': 0.9444,
    'joint_0.95_2': 0.7604,
    'joint_0_3': 0.9757,
    'joint_0.75_3': 0.9757,
    'joint_0.8_3': 0.9757,
    'joint_0.85_3': 0.9688,
    'joint_0.9_3': 0.9583,
    'joint_0.95_3': 0.7951,
    'joint_0_4': 0.9757,
    'joint_0.75_4': 0.9792,
    'joint_0.8_4': 0.9757,
    'joint_0.85_4': 0.9653,
    'joint_0.9_4': 0.9549,
    'joint_0.95_4': 0.8576,
    'joint_0_6': 0.9757,
    'joint_0.75_6': 0.9722,
    'joint_0.8_6': 0.9722,
    'joint_0.85_6': 0.9653,
    'joint_0.9_6': 0.9618,
    'joint_0.95_6': 0.8194,
    'joint_0_8': 0.9757,
    'joint_0.75_8': 0.9757,
    'joint_0.8_8': 0.9757,
    'joint_0.85_8': 0.9688,
    'joint_0.9_8': 0.9549,
    'joint_0.95_8': 0.816,
    'joint_0.925_2': 0.8924,
    'joint_0.925_3': 0.941,
    'joint_0.925_4': 0.9514,
    'joint_0.925_6': 0.9514,
    'joint_0.925_8': 0.9444,
    'joint_0.975_2': 0.5729,
    'joint_0.975_3': 0.5938,
    'joint_0.975_4': 0.5972,
    'joint_0.975_6': 0.5972,
    'joint_0.975_8': 0.5903,
    'seq_0.925_2': 0.9132,
    'seq_0.925_3': 0.9271,
    'seq_0.925_4': 0.9375,
    'seq_0.925_6': 0.9618,
    'seq_0.925_8': 0.9583,
    'seq_0.975_2': 0.559,
    'seq_0.975_3': 0.5694,
    'seq_0.975_4': 0.5521,
    'seq_0.975_6': 0.5868,
    'seq_0.975_8': 0.5382,
}

# Very slow for many datapoints.  Fastest for many costs, most readable


def is_pareto_efficient_dumb(costs):
  """
  Find the pareto-efficient points
  :param costs: An (n_points, n_costs) array
  :return: A (n_points, ) boolean array, indicating whether each point is Pareto efficient
  """
  is_efficient = np.ones(costs.shape[0], dtype=bool)
  for i, c in enumerate(costs):
    is_efficient[i] = np.all(np.any(costs[:i] > c, axis=1)) and np.all(
        np.any(costs[i + 1:] > c, axis=1))
  return is_efficient


# quant
y_quant = []
x_quant = []
x_quant_min = []
x_quant_edp = []

x_quant_cp = []
x_quant_edp_cp = []

x_quant_ubm = []
x_quant_edp_ubm = []

x_quant_uop = []
x_quant_edp_uop = []

for i in [2, 3, 4, 6, 8]:
  x_quant.append(energy_dict_rle['joint_.0_mean_' + str(i)][0] / 1000)
  x_quant_min.append(energy_dict_uop['joint_.0_min_' + str(i)][0] / 1000)
  x_quant_edp.append((energy_dict_rle['joint_.0_mean_' + str(i)]
                     [0] / 1000) * energy_dict_rle['joint_.0_mean_' + str(i)][1])

  x_quant_cp.append(energy_dict_cp['joint_.0_mean_' + str(i)][0] / 1000)
  x_quant_edp_cp.append((energy_dict_cp['joint_.0_mean_' + str(i)]
                        [0] / 1000) * energy_dict_cp['joint_.0_mean_' + str(i)][1])

  x_quant_ubm.append(energy_dict_ubm['joint_.0_mean_' + str(i)][0] / 1000)
  x_quant_edp_ubm.append((energy_dict_ubm['joint_.0_mean_' + str(
      i)][0] / 1000) * energy_dict_ubm['joint_.0_mean_' + str(i)][1])

  x_quant_uop.append(energy_dict_uop['joint_.0_mean_' + str(i)][0] / 1000)
  x_quant_edp_uop.append((energy_dict_uop['joint_.0_mean_' + str(
      i)][0] / 1000) * energy_dict_uop['joint_.0_mean_' + str(i)][1])

  y_quant.append(acc_dict['joint_0_' + str(i)] * 100)

# prune

y_prune = []

x_prune = []
x_prune_min = []
x_prune_edp = []

x_prune_cp = []
x_prune_edp_cp = []

x_prune_ubm = []
x_prune_edp_ubm = []

x_prune_uop = []
x_prune_edp_uop = []
for i in [.0, .75, .8, .85, .9, .925, .95, .975]:
  x_prune.append(energy_dict_rle['joint_' + str(i)[1:] + '_mean_8'][0] / 1000)
  x_prune_min.append(
      energy_dict_uop['joint_' + str(i)[1:] + '_min_8'][0] / 1000)
  x_prune_edp.append((energy_dict_rle['joint_' + str(i)[1:] + '_mean_8']
                     [0] / 1000) * energy_dict_rle['joint_' + str(i)[1:] + '_mean_8'][1])

  x_prune_cp.append(
      energy_dict_cp['joint_' + str(i)[1:] + '_mean_8'][0] / 1000)
  x_prune_edp_cp.append((energy_dict_cp['joint_' + str(i)[1:] + '_mean_8']
                        [0] / 1000) * energy_dict_cp['joint_' + str(i)[1:] + '_mean_8'][1])

  x_prune_ubm.append(
      energy_dict_ubm['joint_' + str(i)[1:] + '_mean_8'][0] / 1000)
  x_prune_edp_ubm.append((energy_dict_ubm['joint_' + str(i)[1:] + '_mean_8']
                         [0] / 1000) * energy_dict_ubm['joint_' + str(i)[1:] + '_mean_8'][1])

  x_prune_uop.append(
      energy_dict_uop['joint_' + str(i)[1:] + '_mean_8'][0] / 1000)
  x_prune_edp_uop.append((energy_dict_uop['joint_' + str(i)[1:] + '_mean_8']
                         [0] / 1000) * energy_dict_uop['joint_' + str(i)[1:] + '_mean_8'][1])

  if i == 0:
    y_prune.append(acc_dict['joint_' + str(i)[0] + '_8'] * 100)
  else:
    y_prune.append(acc_dict['joint_' + str(i) + '_8'] * 100)

# seq
x_seq = []
y_seq = []
x_seq_edp = []
for i in [2, 3, 4, 6, 8]:
  for j in [.75, .8, .85, .9, .925, .95, .975]:
    x_seq.append(energy_dict_rle['seq_' + str(j)
                 [1:] + '_mean_' + str(i)][0] / 1000)
    x_seq_edp.append((energy_dict_rle['seq_' + str(j)[1:] + '_mean_' + str(
        i)][0] / 1000) * energy_dict_rle['seq_' + str(j)[1:] + '_mean_' + str(i)][1])
    y_seq.append(acc_dict['seq_' + str(j) + '_' + str(i)] * 100)

# joint
x_joint = []
y_joint = []
x_joint_edp = []
for i in [2, 3, 4, 6, 8]:
  for j in [.75, .8, .85, .9, .925, .95, .975]:
    x_joint.append(energy_dict_rle['joint_'
                   + str(j)[1:] + '_mean_' + str(i)][0] / 1000)
    x_joint_edp.append((energy_dict_rle['joint_' + str(j)[1:] + '_mean_' + str(
        i)][0] / 1000) * energy_dict_rle['joint_' + str(j)[1:] + '_mean_' + str(i)][1])
    y_joint.append(acc_dict['joint_' + str(j) + '_' + str(i)] * 100)


idx_pareto_joint = is_pareto_efficient_dumb(
    np.vstack([x_joint, 100 - np.array(y_joint)]).transpose())
joint_sort_idx = np.argsort(np.array(x_joint)[idx_pareto_joint])
idx_pareto_seq = is_pareto_efficient_dumb(
    np.vstack([x_seq, 100 - np.array(y_seq)]).transpose())
seq_sort_idx = np.argsort(np.array(x_seq)[idx_pareto_seq])


idx_pareto_joint_edp = is_pareto_efficient_dumb(
    np.vstack([x_joint_edp, 100 - np.array(y_joint)]).transpose())
joint_sort_idx_edp = np.argsort(np.array(x_joint_edp)[idx_pareto_joint_edp])
idx_pareto_seq_edp = is_pareto_efficient_dumb(
    np.vstack([x_seq_edp, 100 - np.array(y_seq)]).transpose())
seq_sort_idx_edp = np.argsort(np.array(x_seq_edp)[idx_pareto_seq_edp])

font_size = 23
gen_linewidth = 2.5
plt.rc('font', family='Helvetica', weight='bold')
fig, ax = plt.subplots(nrows=2, figsize=(7.2, 6.5 * 2))


for tick in ax[0].xaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')
for tick in ax[0].yaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')

ax[0].spines['top'].set_visible(False)
ax[0].spines['right'].set_visible(False)

# change all spines
for axis in ['bottom', 'left']:
  ax[0].spines[axis].set_linewidth(gen_linewidth)

# increase tick width
ax[0].tick_params(width=gen_linewidth)
ax[0].tick_params(axis='both', which='major', labelsize=font_size)


[label.set_fontweight('bold') for label in ax[0].get_yticklabels()]


ax[0].scatter(x_joint, y_joint, marker='X', linewidths=0,
              s=120, color='m', label='Jointly')
ax[0].scatter(x_seq, y_seq, marker='X', linewidths=0,
              s=120, color='g', label='Cumulative')
ax[0].scatter(x_quant, y_quant, marker='X', linewidths=0,
              s=120, color='b', label='Quantization')
ax[0].scatter(x_prune, y_prune, marker='X', linewidths=0,
              s=120, color='r', label='Pruning')


axins = zoomed_inset_axes(ax[0], 3., loc=4, bbox_to_anchor=(
    1700, 2350, 400, 500), axes_kwargs={"facecolor": "lightgray"})


# change all spines
for axis in ['bottom', 'left', 'top', 'right']:
  axins.spines[axis].set_linewidth(gen_linewidth) 

# increase tick width
axins.tick_params(width=gen_linewidth)
axins.tick_params(axis='both', which='major', labelsize=font_size - 5)

axins.set_ylim(94, 99)
axins.set_xlim(8.5, 11.8)


axins.scatter(x_joint, y_joint, marker='X', linewidths=0,
              s=120, color='m', label='Jointly')
axins.scatter(x_seq, y_seq, marker='X', linewidths=0,
              s=120, color='g', label='Cumulative')
axins.scatter(x_quant, y_quant, marker='X', linewidths=0,
              s=120, color='b', label='Quantization')
axins.scatter(x_prune, y_prune, marker='X', linewidths=0,
              s=120, color='r', label='Pruning')

axins.plot(np.array(x_seq)[idx_pareto_seq][seq_sort_idx], np.array(
    y_seq)[idx_pareto_seq][seq_sort_idx], color='g', lw=gen_linewidth)
axins.plot(np.array(x_joint)[idx_pareto_joint][joint_sort_idx], np.array(
    y_joint)[idx_pareto_joint][joint_sort_idx], color='m', lw=gen_linewidth)

mark_inset(ax[0], axins, loc1=3, loc2=1, fc="none", ec=".0", lw=gen_linewidth)

ax[0].set_ylabel("Accuracy (%)", fontsize=font_size, fontweight='bold')
ax[0].set_xlabel("Energy mJ", fontsize=font_size, fontweight='bold')

# 
axins.annotate('Ternary',
               xy=(9.593, 97.57),
               xytext=(.7, .78), textcoords='figure fraction',
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='right', verticalalignment='top', fontsize=font_size, zorder=1000000)


ax[0].annotate('80% Pruning',
               xy=(13.97748, 97.57),
               xytext=(.6, .825), textcoords='figure fraction',
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='right', verticalalignment='top', fontsize=font_size, zorder=1000000)


axins.annotate('3 Bits + 80%',
               xy=(10.94184, 97.57),
               xytext=(.95, .85), textcoords='figure fraction',
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='right', verticalalignment='top', fontsize=font_size, zorder=1000000)

axins.annotate('3 Bits + 80%',
               xy=(11.0322100, 97.57),
               xytext=(.95, .85), textcoords='figure fraction',
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='right', verticalalignment='top', fontsize=font_size, zorder=1000000)


for tick in ax[1].xaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')
for tick in ax[1].yaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')

ax[1].spines['top'].set_visible(False)
ax[1].spines['right'].set_visible(False)

# change all spines
for axis in ['bottom', 'left']:
  ax[1].spines[axis].set_linewidth(gen_linewidth)

# increase tick width
ax[1].tick_params(width=gen_linewidth)
ax[1].tick_params(axis='both', which='major', labelsize=font_size)

[label.set_fontweight('bold') for label in ax[1].get_yticklabels()]

ax[1].set_ylabel("Accuracy (%)", fontsize=font_size, fontweight='bold')
ax[1].set_xlabel("Energy-Delay Product (EDP)",
                 fontsize=font_size, fontweight='bold')

ax[0].axhline(y=97.57000, color='k', alpha=.5, lw=1., linestyle='--')
ax[0].text(19.1, 98, 'Acc. 97.57%', color='k', alpha=.5, fontsize=12)
ax[1].axhline(y=97.57000, color='k', alpha=.5, lw=1., linestyle='--')
ax[1].text(2.75e8, 98, 'Acc. 97.57%', color='k', alpha=.5, fontsize=12)


ax[1].scatter(x_joint_edp, y_joint, marker='X', linewidths=0,
              s=120, color='m', label='Jointly')
ax[1].scatter(x_seq_edp, y_seq, marker='X', linewidths=0,
              s=120, color='g', label='Cumulative')
ax[1].scatter(x_quant_edp, y_quant, marker='X', linewidths=0,
              s=120, color='b', label='Quantization')
ax[1].scatter(x_prune_edp, y_prune, marker='X', linewidths=0,
              s=120, color='r', label='Pruning')


axins = zoomed_inset_axes(ax[1], 3., loc=4, bbox_to_anchor=(
    1700, 550, 400, 500), axes_kwargs={"facecolor": "lightgray"})


# change all spines
for axis in ['bottom', 'left', 'top', 'right']:
  axins.spines[axis].set_linewidth(gen_linewidth)

# increase tick width
axins.tick_params(width=gen_linewidth)
axins.tick_params(axis='both', which='major', labelsize=font_size - 5)

axins.set_ylim(94, 99)
axins.set_xlim(.65 * 1e8, 1.3 * 1e8)


axins.scatter(x_joint_edp, y_joint, marker='X', linewidths=0,
              s=120, color='m', label='Jointly')
axins.scatter(x_seq_edp, y_seq, marker='X', linewidths=0,
              s=120, color='g', label='Cumulative')
axins.scatter(x_quant_edp, y_quant, marker='X', linewidths=0,
              s=120, color='b', label='Quantization')
axins.scatter(x_prune_edp, y_prune, marker='X', linewidths=0,
              s=120, color='r', label='Pruning')


axins.plot(np.array(x_seq_edp)[idx_pareto_seq_edp][seq_sort_idx_edp], np.array(
    y_seq)[idx_pareto_seq_edp][seq_sort_idx_edp], color='g', lw=gen_linewidth)
axins.plot(np.array(x_joint_edp)[idx_pareto_joint_edp][joint_sort_idx_edp], np.array(
    y_joint)[idx_pareto_joint_edp][joint_sort_idx_edp], color='m', lw=gen_linewidth)

mark_inset(ax[1], axins, loc1=3, loc2=1, fc="none", ec=".0", lw=gen_linewidth)

axins.annotate('Ternary',
               xy=(85171997.301, 97.57),
               xytext=(.6, .78 - .45), textcoords='figure fraction',
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='center', verticalalignment='top', fontsize=font_size, zorder=1000000)


ax[1].annotate('80% Pruning',
               xy=(135147695.0208, 97.57),
               xytext=(.55, .825 - .45), textcoords='figure fraction',
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='right', verticalalignment='top', fontsize=font_size, zorder=1000000)


axins.annotate('3 Bits + 80%',
               xy=(104653231.98813003, 97.57),
               xytext=(.79, .85 - .47), textcoords='figure fraction',
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='center', verticalalignment='top', fontsize=font_size, zorder=1000000)

axins.annotate('3 Bits + 80%',
               xy=(100792466.08704, 97.57),
               xytext=(.79, .85 - .47), textcoords='figure fraction',
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='center', verticalalignment='top', fontsize=font_size, zorder=1000000)


ax[0].legend(
    bbox_to_anchor=(-0.1, 1.15, 1.0, .1),
    # bbox_transform=plt.gcf().transFigure,
    # mode="expand",
    loc="upper center",
    ncol=2,
    frameon=False,
    prop={'weight': 'bold', 'size': font_size},
)


plt.tight_layout()
plt.savefig('overview.png', dpi=300)
plt.close()

##############
# Sparsity illustration
##############

my_data = pd.read_csv('workload_all.txt')

layer_names = ['Conv1', 'Conv2', 'Conv3', 'Conv4', 'TCJA11',
               'TCJA12', 'Conv5', 'TCJA21', 'TCJA22', 'Dense1', 'Dense2']

# get 2 bit
quant_sparse_mean = []
for ln in layer_names:
  quant_sparse_mean.append(
      (1 - float(my_data[my_data["name"] == ln + "_joint_.0_mean_2"]["weights"])) * 100)


# get 95%
prune_sparse_mean = []
for ln in layer_names:
  prune_sparse_mean.append(
      (1 - float(my_data[my_data["name"] == ln + "_joint_.8_mean_8"]["weights"])) * 100)


# get some mix joint
joint_sparse_mean = []
for ln in layer_names:
  joint_sparse_mean.append(
      (1 - float(my_data[my_data["name"] == ln + "_joint_.8_mean_3"]["weights"])) * 100)


# get some mix seq
seq_sparse_mean = []
for ln in layer_names:
  seq_sparse_mean.append(
      (1 - float(my_data[my_data["name"] == ln + "_seq_.8_mean_3"]["weights"])) * 100)


plt.rc('font', family='Helvetica', weight='bold')
fig, ax = plt.subplots(figsize=(7.2, 6.5))


for tick in ax.xaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')
for tick in ax.yaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# change all spines
for axis in ['bottom', 'left']:
  ax.spines[axis].set_linewidth(gen_linewidth)

# increase tick width
ax.tick_params(width=gen_linewidth)
ax.tick_params(axis='both', which='major', labelsize=font_size)

ax.bar(np.arange(11) + .6, joint_sparse_mean, width=.2, color='m',
       edgecolor='k', label='Jointly (3b + 80%)', linewidth=1.5)
ax.bar(np.arange(11) + .4, seq_sparse_mean, width=.2, color='g',
       edgecolor='k', label='Cumulative (3b + 80%)', linewidth=1.5)
ax.bar(np.arange(11), quant_sparse_mean, width=.2, color='b',
       edgecolor='k', label='Quant. (Ternary)', linewidth=1.5)
ax.bar(np.arange(11) + .2, prune_sparse_mean, width=.2, color='r',
       edgecolor='k', label='Pruning (80%)', linewidth=1.5)


ax.set_ylabel("Sparsity (%)", fontsize=font_size, fontweight='bold')

plt.legend(
    bbox_to_anchor=(0., .98, .8, .1),
    loc="lower center",
    ncol=2,
    frameon=False,
    prop={'weight': 'bold', 'size': font_size - 4},
    columnspacing=0.2,
)

ax.set_xticks([.0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.], ['Conv', 'Conv', 'Conv', 'Conv', 'TCJA-t',
              'TCJA-c', 'Conv', 'TCJA-t', 'TCJA-c', 'Dense', 'Dense'], rotation=-60, ha='left', rotation_mode='anchor')

# ax.set_xlabel("", fontsize=font_size, fontweight='bold')
plt.tight_layout()
plt.savefig('sparsity.png', dpi=300)
plt.close()


##############
# Encoding Schemes
##############


# get sparsity
my_data = pd.read_csv('workload_all.txt')

layer_names = ['Conv1', 'Conv2', 'Conv3', 'Conv4', 'TCJA11',
               'TCJA12', 'Conv5', 'TCJA21', 'TCJA22', 'Dense1', 'Dense2']

# get sparsity for quant
q_sparse = []

for q_lvl in [2, 3, 4, 6, 8]:
  quant_sparse_mean = []
  param_num = []
  for ln in layer_names:
    quant_sparse_mean.append(
        (1 - float(my_data[my_data["name"] == ln + "_joint_.0_mean_" + str(q_lvl)]["weights"])) * 100)
    if 'Dense' in ln:
      param_num.append(float(my_data[my_data["name"] == ln + "_joint_.0_mean_" + str(q_lvl)]['M'])
                       * float(my_data[my_data["name"] == ln + "_joint_.0_mean_" + str(q_lvl)]['C']))
    else:
      param_num.append(float(my_data[my_data["name"] == ln + "_joint_.0_mean_" + str(q_lvl)]['M']) * float(my_data[my_data["name"] == ln + "_joint_.0_mean_" + str(q_lvl)]['C'])
                       * float(my_data[my_data["name"] == ln + "_joint_.0_mean_" + str(q_lvl)]['R']) * float(my_data[my_data["name"] == ln + "_joint_.0_mean_" + str(q_lvl)]['S']))

  q_sparse.append(np.average(quant_sparse_mean, weights=(
      np.array(param_num) / np.sum(param_num))))


q_prune = []
for i in [".0", ".75", ".8", ".85", ".9", ".925", ".95", ".975"]:
  prune_sparse_mean = []
  for ln in layer_names:
    prune_sparse_mean.append(
        (1 - float(my_data[my_data["name"] == ln + "_joint_" + i + "_mean_8"]["weights"])) * 100)

  q_prune.append(np.average(prune_sparse_mean, weights=(
      np.array(param_num) / np.sum(param_num))))

plt.rc('font', family='Helvetica', weight='bold')
fig, ax = plt.subplots(nrows=2, figsize=(7.2, 6.5 * 2))


for tick in ax[0].xaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')
for tick in ax[0].yaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')

ax[0].spines['top'].set_visible(False)
ax[0].spines['right'].set_visible(False)

# change all spines
for axis in ['bottom', 'left']:
  ax[0].spines[axis].set_linewidth(gen_linewidth)

# increase tick width
ax[0].tick_params(width=gen_linewidth)
ax[0].tick_params(axis='both', which='major', labelsize=font_size)


[label.set_fontweight('bold') for label in ax[0].get_yticklabels()]


hq_rle, = ax[0].plot(q_sparse, x_quant, marker='X',
                     lw=gen_linewidth, color="b", markersize=12, label='RLE')
hq_cp, = ax[0].plot(q_sparse, x_quant_cp, marker='d',
                    lw=gen_linewidth, color="#0077b6", markersize=12, label='CP')
hq_uop, = ax[0].plot(q_sparse, x_quant_uop, marker='o',
                     lw=gen_linewidth, color="#00b4d8", markersize=12, label='UOP')
hq_ubm, = ax[0].plot(q_sparse, x_quant_ubm, marker='*',
                     lw=gen_linewidth, color="#90e0ef", markersize=12, label='UBM')


hp_rle, = ax[0].plot(q_prune[1:], x_prune[1:], marker='X',
                     lw=gen_linewidth, color="r", markersize=12, label='RLE')
hp_cp, = ax[0].plot(q_prune[1:], x_prune_cp[1:], marker='d',
                    lw=gen_linewidth, color="#ff5252", markersize=12, label='CP')
hp_uop, = ax[0].plot(q_prune[1:], x_prune_uop[1:], marker='o',
                     lw=gen_linewidth, color="#ff7b7b", markersize=12, label='UOP')
hp_ubm, = ax[0].plot(q_prune[1:], x_prune_ubm[1:], marker='*',
                     lw=gen_linewidth, color="#ffbaba", markersize=12, label='UBM')


ax[0].add_patch(mpl.patches.Ellipse((q_sparse[0], np.mean([x_quant[0], x_quant_ubm[0]])),
                5, 5., fill=False, lw=gen_linewidth, ls='--', color='k', zorder=1000))
ax[0].annotate('Ternary',
               xy=(q_sparse[0], 14.),
               xytext=(q_sparse[0] + 5 + 1, 17),
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='right', verticalalignment='top', fontsize=font_size)


ax[0].add_patch(mpl.patches.Ellipse((q_prune[2], np.mean([x_prune[2], x_prune_ubm[2]])),
                5, 6., fill=False, lw=gen_linewidth, ls='--', color='k', zorder=1000))
ax[0].annotate('80% Pruning',
               xy=(q_prune[2], 19.15),
               xytext=(q_prune[2] + 15, 23.5),
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='right', verticalalignment='top', fontsize=font_size)


ax[0].set_ylabel("Energy mJ", fontsize=font_size, fontweight='bold')
ax[0].set_xlabel("Sparsity (%)", fontsize=font_size, fontweight='bold')


for tick in ax[1].xaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')
for tick in ax[1].yaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')

ax[1].spines['top'].set_visible(False)
ax[1].spines['right'].set_visible(False)

# change all spines
for axis in ['bottom', 'left']:
  ax[1].spines[axis].set_linewidth(gen_linewidth)

# increase tick width
ax[1].tick_params(width=gen_linewidth)
ax[1].tick_params(axis='both', which='major', labelsize=font_size)

[label.set_fontweight('bold') for label in ax[1].get_yticklabels()]

ax[1].set_ylabel("Energy-Delay Product (EDP)",
                 fontsize=font_size, fontweight='bold')
ax[1].set_xlabel("Sparsity (%)", fontsize=font_size, fontweight='bold')


ax[1].plot(q_sparse, x_quant_edp, marker='X', lw=gen_linewidth,
           color="b", markersize=12, label='RLE')
ax[1].plot(q_sparse, x_quant_edp_cp, marker='d', lw=gen_linewidth,
           color="#0077b6", markersize=12, label='CP')
ax[1].plot(q_sparse, x_quant_edp_uop, marker='o', lw=gen_linewidth,
           color="#00b4d8", markersize=12, label='UOP')
ax[1].plot(q_sparse, x_quant_edp_ubm, marker='*', lw=gen_linewidth,
           color="#90e0ef", markersize=12, label='UBM')


ax[1].plot(q_prune[1:], x_prune_edp[1:], marker='X',
           lw=gen_linewidth, color="r", markersize=12, label='RLE')
ax[1].plot(q_prune[1:], x_prune_edp_cp[1:], marker='d',
           lw=gen_linewidth, color="#ff5252", markersize=12, label='CP')
ax[1].plot(q_prune[1:], x_prune_edp_uop[1:], marker='o',
           lw=gen_linewidth, color="#ff7b7b", markersize=12, label='UOP')
ax[1].plot(q_prune[1:], x_prune_edp_ubm[1:], marker='*',
           lw=gen_linewidth, color="#ffbaba", markersize=12, label='UBM')

ax[1].add_patch(mpl.patches.Ellipse((q_sparse[0], np.mean([x_quant_edp[0], x_quant_edp_ubm[0]])),
                5, .57 * 1e8, fill=False, lw=gen_linewidth, ls='--', color='k', zorder=1000))
ax[1].annotate('Ternary',
               xy=(q_sparse[0], 1.3e8),
               xytext=(q_sparse[0] + 5 + 1, 2e8),
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='right', verticalalignment='top', fontsize=font_size)


ax[1].add_patch(mpl.patches.Ellipse((q_prune[2], np.mean([x_prune_edp[2], x_prune_edp_ubm[2]])),
                5, .7 * 1e8, fill=False, lw=gen_linewidth, ls='--', color='k', zorder=1000))
ax[1].annotate('80% Pruning',
               xy=(q_prune[2], 1.9e8),
               xytext=(q_prune[2] + 15, 2.8e8),
               arrowprops=dict(width=gen_linewidth,
                               facecolor='black', headwidth=10),
               horizontalalignment='right', verticalalignment='top', fontsize=font_size)

first_legend = ax[0].legend(
    handles=[hq_rle, hq_cp, hp_rle, hp_cp, hq_uop, hq_ubm, hp_uop, hp_ubm, ],
    labels=['', '', 'RLE', 'CP', '', '', 'UOP', 'UBM'],
    borderpad=0.8, handletextpad=1., columnspacing=0.3,
    bbox_to_anchor=(-.1, 1.27, 1.1, .1),
    title="Compression Scheme",
    title_fontsize=font_size,
    loc="upper center",
    ncol=4,
    frameon=False,
    prop={'weight': 'bold', 'size': font_size},
)

ax[0].add_artist(first_legend)
custom_lines = [mpl.lines.Line2D([0], [0], color='b', lw=gen_linewidth, label='Quantization'),
                mpl.lines.Line2D([0], [0], color='r', lw=gen_linewidth, label='Pruning'), ]


ax[0].legend(
    handles=custom_lines,
    bbox_to_anchor=(-.1, 1.33, 1.1, .1),
    loc="upper center",
    ncol=4,
    frameon=False,
    prop={'weight': 'bold', 'size': font_size},
)

plt.tight_layout()
plt.savefig('encoding.png', dpi=300)
plt.close()


########
# mean vs min
#########


plt.rc('font', family='Helvetica', weight='bold')
fig, ax = plt.subplots(figsize=(7.2, 6.5))


for tick in ax.xaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')
for tick in ax.yaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# change all spines
for axis in ['bottom', 'left']:
  ax.spines[axis].set_linewidth(gen_linewidth)

# increase tick width
ax.tick_params(width=gen_linewidth)
ax.tick_params(axis='both', which='major', labelsize=font_size)


ax.plot(q_sparse, x_quant_uop, marker='x', lw=gen_linewidth,
        color="b", markersize=12, label='Quantization Average')
ax.plot(q_prune[1:], x_prune_uop[1:], marker='x', lw=gen_linewidth,
        color="r", markersize=12, label='Pruning Average')

ax.plot(q_sparse, x_quant_min, marker='x', lw=gen_linewidth, color="b",
        markersize=12, linestyle='dashed', label='Quantization Max')
ax.plot(q_prune[1:], x_prune_min[1:], marker='x', lw=gen_linewidth,
        color="r", markersize=12, linestyle='dashed', label='Pruning Max')


ax.set_ylabel("Energy mJ", fontsize=font_size, fontweight='bold')
ax.set_xlabel("Sparsity (%)", fontsize=font_size, fontweight='bold')

plt.legend(
    bbox_to_anchor=(0., .98, .8, .1),
    loc="lower center",
    ncol=2,
    frameon=False,
    prop={'weight': 'bold', 'size': font_size - 4},
    columnspacing=0.2,
)


# ax.set_xlabel("", fontsize=font_size, fontweight='bold')
plt.tight_layout()
plt.savefig('min_mean.png', dpi=300)
plt.close()


################################################
# Dense models
################################################


plt.rc('font', family='Helvetica', weight='bold')
fig, ax = plt.subplots(nrows=1, figsize=(7.2, 6.5))


for tick in ax.xaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')
for tick in ax.yaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# change all spines
for axis in ['bottom', 'left']:
  ax.spines[axis].set_linewidth(gen_linewidth)

# increase tick width
ax.tick_params(width=gen_linewidth)
ax.tick_params(axis='both', which='major', labelsize=font_size)


[label.set_fontweight('bold') for label in ax.get_yticklabels()]


ax.plot([2, 3, 4, 6, 8], np.array([16562.17, 17812.07, 18140.24, 18445.72, 20325.56])
        / 1000, marker='X', lw=gen_linewidth, color="g", markersize=12, label='Quant RLE')

ax.plot([2, 3, 4, 6, 8], np.array([18430.25, 19588.85, 19968.44, 20263.66, 22087.55])
        / 1000, marker='*', lw=gen_linewidth, color="g", markersize=12, label='Quant UOP')

ax.plot([2, 3, 4, 6, 8], x_quant, marker='X', lw=gen_linewidth,
        color="b", markersize=12, label='Quant+Sparse RLE')

ax.plot([2, 3, 4, 6, 8], x_quant_uop, marker='*', lw=gen_linewidth,
        color="b", markersize=12, label='Quant+Sparse UOP')


ax.annotate('67.9%', (2.1, 18430.25 / 1000 + .5), horizontalalignment='center',
            verticalalignment='bottom', fontsize=14, zorder=1000000, alpha=.75)
ax.annotate('31.6%', (3, 19588.85 / 1000 + .5), horizontalalignment='center',
            verticalalignment='bottom', fontsize=14, zorder=1000000, alpha=.75)
ax.annotate('15.1%', (4, 19968.44 / 1000 + .5), horizontalalignment='center',
            verticalalignment='bottom', fontsize=14, zorder=1000000, alpha=.75)
ax.annotate('3.6%', (6, x_quant[-2] + .5), horizontalalignment='center',
            verticalalignment='bottom', fontsize=14, zorder=1000000, alpha=.75)
ax.annotate('0.9%', (8, x_quant[-1] + .5), horizontalalignment='center',
            verticalalignment='bottom', fontsize=14, zorder=1000000, alpha=.75)


ax.set_ylabel("Energy mJ", fontsize=font_size, fontweight='bold')
ax.set_xlabel("Bit Width", fontsize=font_size, fontweight='bold')


custom_lines = [
    mpl.lines.Line2D([0], [0], color='k', marker='X',
                     markersize=12, lw=gen_linewidth, label='RLE'),
    mpl.lines.Line2D([0], [0], color='k', marker='*',
                     markersize=12, lw=gen_linewidth, label='UOP'),
]
lg1 = ax.legend(
    handles=custom_lines,
    bbox_to_anchor=(.5, .3, .4, .15),
    loc="upper center",
    title='Sparsity Encoding',
    title_fontsize=font_size,
    ncol=1,
    frameon=False,
    prop={'weight': 'bold', 'size': font_size},
)


custom_lines = [mpl.lines.Line2D([0], [0], color='g', lw=gen_linewidth, label='No Wgt. Sparsity'),
                mpl.lines.Line2D([0], [0], color='b', lw=gen_linewidth,
                                 label='Quantization Induced Wgt. Sparsity'),
                ]

ax.legend(
    handles=custom_lines,
    bbox_to_anchor=(-.1, 1.0, 1., .05),
    loc="lower center",
    ncol=1,
    frameon=False,
    prop={'weight': 'bold', 'size': font_size},
)


ax.set_xticks([2, 3, 4, 6, 8], [2, 3, 4, 6, 8])
ax.add_artist(lg1)

plt.tight_layout()
plt.savefig('quant_prune_advantage.png', dpi=300)
plt.close()


################################################
# Energy Breakdown
################################################


plt.rc('font', family='Helvetica', weight='bold')
fig, ax = plt.subplots(nrows=2, figsize=(
    7.2, 6.5), sharex=True, gridspec_kw={'height_ratios': [1, 4]})
# f, (ax, ax2) = plt.subplots(2, 1, sharex=True)

# for tick in ax[0].xaxis.get_major_ticks():
#   tick.label1.set_fontweight('bold')
for tick in ax[0].yaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')

ax[0].spines['top'].set_visible(False)
ax[0].spines['right'].set_visible(False)
ax[0].spines['bottom'].set_visible(False)

# change all spines
for axis in ['left']:
  ax[0].spines[axis].set_linewidth(gen_linewidth)

# increase tick width
ax[0].tick_params(width=gen_linewidth, bottom=False,)
ax[0].tick_params(axis='both', which='major',
                  labelsize=font_size, bottom=False,)


[label.set_fontweight('bold') for label in ax[0].get_yticklabels()]


for tick in ax[1].xaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')
for tick in ax[1].yaxis.get_major_ticks():
  tick.label1.set_fontweight('bold')

ax[1].spines['top'].set_visible(False)
ax[1].spines['right'].set_visible(False)

# change all spines
for axis in ['bottom', 'left']:
  ax[1].spines[axis].set_linewidth(gen_linewidth)

# increase tick width
ax[1].tick_params(width=gen_linewidth)
ax[1].tick_params(axis='both', which='major', labelsize=font_size)


[label.set_fontweight('bold') for label in ax[1].get_yticklabels()]


breakdown_data = {
    # ' DummyBuffer  ' : ( 0.  , 0.  , 0.  ),
    ' ifmap_spad  ': (2.47, 1.31, 0.89),
    ' ifmap_spad <==> weights_spad  ': (3.89, 1.94, 2.18),
    ' psum_spad <==> MACs ': (6.4, 4.67, 4.7),
    ' weights_spad  ': (8.81, 16.06, 16),
    ' weights_spad <==> psum_spad ': (10.43, 14.36, 14.84),
    ' MACs  ': (12.54, 2.64, 2.64),
    ' shared_glb  ': (14.38, 6.12, 11.61),
    # ' DummyBuffer <==> ifmap_spad ' : ( 23.33 , 14.08 , 15.2  ),
    ' psum_spad ': (25.31, 12.18, 14.22),
    # ( 23.33 , 14.08 , 15.2  ),
    # ( 26.21 , 16.9  , 18.33 ), 
    ' shared_glb <==> DummyBuffer ': (23.33 + 26.21, 14.08 + 16.9, 15.2 + 18.33),
    ' DRAM <==> shared_glb  ': (63, 30.49, 31.77),
    ' DRAM  ': (457.55, 538.47, 537.22),
}


clist = [
    # ("#BBBBBB",'#BBBBBB'), # DummyBuffer
    ('#EE6677', '#EE6677'),  # ifmap_spad
    ('#EE6677', '#228833'),  # ifmap_spad <==> weights_spad
    ("#AA3377", '#CCBB44'),  # psum_spad <==> MACs
    ('#228833', '#228833'),  # weights_spad
    ('#228833', '#AA3377'),  # weights_spad <==> psum_spad
    ("#CCBB44", '#CCBB44'),  # MACs
    ('#66CCEE', '#66CCEE'),  # shared_glb 
    # ('#BBBBBB', '#EE6677'), #  DummyBuffer <==> ifmap_spad
    ("#AA3377", '#AA3377'),  # psum_spad
    ('#66CCEE', '#EE6677'),  # shared_glb <==> DummyBuffer
    ('#4477AA', '#66CCEE'),  # DRAM <==> shared_glb
    ("#4477AA", '#4477AA'),  # DRAM
]

p123 = []

plt.rcParams['hatch.linewidth'] = 8.5
running_bottom = [0, 0, 0]
idx = 0
for k, v in breakdown_data.items():
  p_temp = ax[0].bar([1, 2, 3], v, 0.5, bottom=running_bottom, color=clist[idx]
                     [0], hatch='/', edgecolor=clist[idx][1], linewidth=0, zorder=0)
  ax[0].bar([1, 2, 3], v, 0.5, bottom=running_bottom,
            color='none', edgecolor='k', linewidth=1, zorder=10000)
  running_bottom[0] += v[0]
  running_bottom[1] += v[1]
  running_bottom[2] += v[2]
  idx += 1
  p123.append(p_temp)


p123 = []
running_bottom = [0, 0, 0]
idx = 0
for k, v in breakdown_data.items():
  p_temp = ax[1].bar([1, 2, 3], v, 0.5, bottom=running_bottom, color=clist[idx]
                     [0], hatch='/', edgecolor=clist[idx][1], linewidth=0, zorder=0)
  ax[1].bar([1, 2, 3], v, 0.5, bottom=running_bottom,
            color='none', edgecolor='k', linewidth=1, zorder=10000)
  running_bottom[0] += v[0]
  running_bottom[1] += v[1]
  running_bottom[2] += v[2]
  idx += 1
  p123.append(p_temp)

ax[0].set_ylim(650, 675)  # outliers only
ax[1].set_ylim(0, 200)  # most of the data


ax[1].set_ylabel("pJ/Compute", fontsize=font_size, fontweight='bold')
# ax[1].yaxis.set_label_coords(-0.22, .7)

d = .021  # how big to make the diagonal lines in axes coordinates
# arguments to pass to plot, just so we don't keep repeating them
kwargs = dict(transform=ax[0].transAxes, color='k', clip_on=False)
ax[0].plot((-d, +d), (-d - .052, +d + .052), lw=gen_linewidth,
           **kwargs)        # top-left diagonal

d = .019
kwargs.update(transform=ax[1].transAxes)  # switch to the bottom axes
ax[1].plot((-d, +d), (1 - d + .002, 1 + d + .002),
           lw=gen_linewidth, **kwargs)  # bottom-left diagonal


lg1 = ax[0].legend(
    reversed([p123[0], p123[3], p123[5], p123[6], p123[7], p123[10], ]),
    reversed([
        # 'DummyBuffer (g)', # 
        'Inpt. Buffer (f)',  # Local Input Buffer
        'Wgt. Buffer (e)',  # Local Weight Buffer 
        'MACs (d)',
        'Shared Act. Buffer (c)',  # Shared Activation Buffer 
        'Output Buffer (b)',  # Local Output Buffer
        'DRAM (a)',
    ]),
    bbox_to_anchor=(1.04, 1.16),
    ncol=1,
    frameon=False,
    title="Ops and Memory",
    title_fontsize=14,
    labelspacing=1.5,
    prop={'weight': 'bold', 'size': 11},
)
lg1._legend_box.align = "left"


for patch in lg1.get_patches():
  patch.set_height(16)
  patch.set_width(30)
  patch.set_x(-4)
  patch.set_y(-4)

lg2 = ax[1].legend(
    reversed([p123[1], p123[2], p123[4], p123[8], p123[9], ]),
    reversed([
        '(f) <-> (e)',  # 'ifmap_spad <-> weights_spad' 
        '(b) <-> (d)',  # 'psum_spad <-> MACs' 
        '(e) <-> (b)',  # 'weights_spad <-> psum_spad' 
        # '(g) <-> (f)', #'DummyBuffer <-> ifmap_spad' 
        '(c) <-> (f)',  # 'shared_glb <-> DummyBuffer' 
        '(a) <-> (c)',  # 'DRAM <-> shared_glb' 
    ]),
    bbox_to_anchor=(1.04, .6),
    ncol=1,
    frameon=False,
    title="Communication",
    title_fontsize=14,
    labelspacing=1.5,
    prop={'weight': 'bold', 'size': 11},
)
lg2._legend_box.align = "left"

for patch in lg2.get_patches():
  patch.set_height(16)
  patch.set_width(30)
  patch.set_x(-4)
  patch.set_y(-4)

ax[1].set_xticklabels(
  ['', 'Ternary', '8b Dense', '8b Sparse'], rotation=-20, ha='left')

# "DAC (25 pJ)",
# "Add (36 pJ)",
# "CIM Cell (140 pJ)",
# "Sigmoid (426 pJ)",
# "Mult (1.57 nJ)",
# "ADC (125.3 nJ)",
# "Sigmoid (420 pJ)",
# "IFMAP (54.75 nJ)",
# "MAC (23.25 nJ)",
# "PSUM (107.86 nJ)",
# "Weights (107.86 nJ)",
# "Global Buffer (1.5143 uJ)",

# ax.legend(
#     handles = custom_lines,
#     bbox_to_anchor=(1., .5, .2, 1.),
#     loc="center left",
#     ncol=1,
#     frameon=False,
#     prop={'weight': 'bold', 'size': font_size},
# )


plt.tight_layout()
plt.subplots_adjust(hspace=.1)
plt.savefig('breakdown.png', dpi=300)
plt.close()


